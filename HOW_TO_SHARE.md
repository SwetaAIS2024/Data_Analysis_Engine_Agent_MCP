# üéØ How Other Users Can Use This System

## Option 1: For Non-Technical Users (Easiest)

### Using the Quick Start Script

**Windows Users:**
1. Double-click `quick_start.bat`
2. Choose option 1 (Start ALL services)
3. Wait 60 seconds
4. Open http://localhost:3001 in your browser
5. Upload CSV and start analyzing!

**Mac/Linux Users:**
1. Open Terminal
2. Navigate to project folder
3. Run: `chmod +x quick_start.sh && ./quick_start.sh`
4. Choose option 1
5. Wait 60 seconds
6. Open http://localhost:3001 in your browser

### Using Docker Only (Simpler)
```bash
# From project root
docker-compose up -d --build

# Wait 30 seconds, then open:
# http://localhost:3001
```

---

## Option 2: For Technical Users

### Clone and Run
```bash
# 1. Clone repository
git clone https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP.git
cd Data_Analysis_Engine_Agent_MCP
git checkout DAA_v2

# 2. Start with Docker
docker-compose up -d --build

# 3. Access
# Frontend: http://localhost:3001
# API: http://localhost:8080/docs
```

### Development Setup
```bash
# Backend
cd services/agent
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8080

# Frontend (new terminal)
cd frontend
npm install
npm start

# Docker Services (new terminal)
docker-compose up -d
```

---

## Option 3: Deploying for Team Use

### A. Docker Image Distribution

**Create Docker Image:**
```bash
# Build a complete image
docker build -t data-analysis-agent:v2 .

# Save image to file
docker save -o data-analysis-agent-v2.tar data-analysis-agent:v2

# Share this .tar file with others
```

**Others Load and Run:**
```bash
# Load image
docker load -i data-analysis-agent-v2.tar

# Run
docker-compose up -d
```

### B. Cloud Deployment

#### AWS (Elastic Container Service)
```bash
# Push to ECR
aws ecr create-repository --repository-name data-analysis-agent
docker tag data-analysis-agent:v2 <account-id>.dkr.ecr.us-east-1.amazonaws.com/data-analysis-agent:v2
docker push <account-id>.dkr.ecr.us-east-1.amazonaws.com/data-analysis-agent:v2

# Deploy using ECS or EC2
```

#### Azure (Container Instances)
```bash
# Push to ACR
az acr create --resource-group myResourceGroup --name myContainerRegistry --sku Basic
az acr login --name myContainerRegistry
docker tag data-analysis-agent:v2 mycontainerregistry.azurecr.io/data-analysis-agent:v2
docker push mycontainerregistry.azurecr.io/data-analysis-agent:v2

# Deploy
az container create --resource-group myResourceGroup --name data-analysis-agent \
  --image mycontainerregistry.azurecr.io/data-analysis-agent:v2 --ports 3001 8080
```

#### Google Cloud (Cloud Run)
```bash
# Push to GCR
gcloud builds submit --tag gcr.io/PROJECT-ID/data-analysis-agent:v2
gcloud run deploy data-analysis-agent --image gcr.io/PROJECT-ID/data-analysis-agent:v2 --platform managed
```

### C. Internal Server Deployment

**Setup on Company Server:**
```bash
# 1. Install Docker on server
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# 2. Clone and deploy
git clone https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP.git
cd Data_Analysis_Engine_Agent_MCP
git checkout DAA_v2
docker-compose up -d --build

# 3. Configure firewall
sudo ufw allow 3001  # Frontend
sudo ufw allow 8080  # Backend API

# 4. Setup reverse proxy (nginx)
# See USER_SETUP_GUIDE.md for nginx config
```

**Access URL:** `http://your-server-ip:3001`

---

## Option 4: Sharing as a Service

### Create User-Friendly Package

**1. Create Distribution Folder:**
```
data-analysis-agent-v2/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ quick_start.bat (Windows)
‚îú‚îÄ‚îÄ quick_start.sh (Linux/Mac)
‚îú‚îÄ‚îÄ README_FOR_USERS.txt
‚îî‚îÄ‚îÄ sample_data.csv
```

**2. Create README_FOR_USERS.txt:**
```
Data Analysis Engine Agent v2.0

QUICK START:
1. Install Docker Desktop: https://www.docker.com/products/docker-desktop
2. Windows: Double-click quick_start.bat
   Mac/Linux: Open terminal, run: chmod +x quick_start.sh && ./quick_start.sh
3. Choose option 1 (Start ALL services)
4. Wait 60 seconds
5. Open http://localhost:3001 in your browser
6. Upload your CSV file
7. Type what you want (e.g., "detect anomalies")
8. Click Analyze!

EXAMPLE PROMPTS:
- "detect anomalies in temperature"
- "cluster customers into 3 groups"
- "forecast sales for next 7 days"

SAMPLE DATA:
Use sample_data.csv to try it out first.

SUPPORT:
GitHub: https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP
Email: your-support@email.com
```

**3. Package and Share:**
```bash
# Create zip file
zip -r data-analysis-agent-v2.zip data-analysis-agent-v2/

# Or create installer (Windows)
# Use tools like Inno Setup or NSIS
```

---

## What Users Need

### Minimum Requirements:
- **RAM**: 4GB
- **Disk**: 2GB free space
- **OS**: Windows 10+, macOS, or Linux
- **Software**: Docker Desktop

### They Don't Need:
- ‚ùå Python knowledge
- ‚ùå Programming skills
- ‚ùå Database setup
- ‚ùå Complex configuration

### What They Get:
- ‚úÖ Web interface at http://localhost:3001
- ‚úÖ Automatic tool selection
- ‚úÖ Real-time results
- ‚úÖ Pipeline visibility
- ‚úÖ CSV upload and download

---

## Usage Instructions for End Users

### Step 1: Prepare Your Data
Create a CSV file with:
- Column headers in first row
- Consistent data types
- (Optional) Timestamp column for time-series
- (Optional) ID column for entities

**Example:**
```csv
timestamp,sensor_id,temperature,humidity
2024-11-01 10:00:00,S001,22.5,45
2024-11-01 10:05:00,S001,23.1,46
```

### Step 2: Start the System
- Windows: Run `quick_start.bat`, choose 1
- Mac/Linux: Run `./quick_start.sh`, choose 1
- Or: `docker-compose up -d`

### Step 3: Access Web Interface
Open: http://localhost:3001

### Step 4: Analyze Data
1. **Upload CSV**: Click "Choose File"
2. **Enter Task**: Type what you want
   - "detect anomalies"
   - "cluster data"
   - "forecast values"
3. **Configure**:
   - Metric Column: Which column to analyze
   - Timestamp: (Optional) For time-series
   - Key Fields: (Optional) Entity IDs
4. **Click "Analyze"**

### Step 5: View Results
- **Pipeline Logs**: See what the system did
- **Tool Results**: Analysis output
- **Visualizations**: Charts and graphs
- **Summary**: Key findings

### Step 6: Stop the System
- Windows: Run `quick_start.bat`, choose 3
- Mac/Linux: Run `./quick_start.sh`, choose 3
- Or: `docker-compose down`

---

## Common Questions

### Q: Do I need to install Python or Node.js?
**A:** No! Docker includes everything. Just install Docker Desktop.

### Q: Will my data leave my computer?
**A:** No. Everything runs locally on your machine (unless you deploy to cloud).

### Q: Can multiple people use this?
**A:** Yes! Deploy to a server and share the URL. Add authentication for security.

### Q: What file formats are supported?
**A:** Currently CSV files. Excel support coming soon.

### Q: How do I add more analysis types?
**A:** See `USER_SETUP_GUIDE.md` for adding custom tools.

### Q: Is this free to use?
**A:** Yes! MIT License - free for personal and commercial use.

---

## Sharing Checklist

### For GitHub:
- ‚úÖ Push to public repository
- ‚úÖ Add clear README with setup instructions
- ‚úÖ Include sample data files
- ‚úÖ Add screenshots/demo video
- ‚úÖ Write issues and documentation

### For Team/Company:
- ‚úÖ Deploy to internal server
- ‚úÖ Configure authentication (JWT)
- ‚úÖ Set up HTTPS/SSL
- ‚úÖ Create user documentation
- ‚úÖ Provide training/demo session
- ‚úÖ Set up support channel

### For Non-Technical Users:
- ‚úÖ Create simple package with quick_start scripts
- ‚úÖ Include one-page quick reference
- ‚úÖ Provide sample CSV files
- ‚úÖ Record video tutorial
- ‚úÖ Test on fresh machine
- ‚úÖ Provide support contact

---

## Support Resources

**Documentation:**
- Full Setup: `USER_SETUP_GUIDE.md`
- API Reference: http://localhost:8080/docs
- Logging Guide: `PIPELINE_LOGGING.md`

**Getting Help:**
- GitHub Issues: Report bugs, ask questions
- Logs: `services/agent/logs/agent_v2.log`
- Docker logs: `docker-compose logs <service>`

**Community:**
- Create GitHub Discussions for Q&A
- Add FAQ section based on common issues
- Share use cases and examples

---

## Example Use Cases to Share

### 1. IoT Sensor Monitoring
"Our factory has 100 temperature sensors. This tool automatically detects when sensors report unusual readings."

### 2. Customer Segmentation
"We use this to group customers by purchase behavior. Just upload customer data and say 'cluster into 3 segments'."

### 3. Sales Forecasting
"Upload historical sales and it predicts next week's numbers. Helps with inventory planning."

### 4. Fraud Detection
"Detects unusual transaction patterns. Upload transactions and say 'detect anomalies'."

### 5. Quality Control
"Monitors production metrics and alerts when values go outside normal ranges."

---

## Next Steps

1. **Try it yourself first** - Make sure everything works
2. **Create sample data** - Provide examples for others
3. **Document your use case** - Show real-world applications
4. **Package it up** - Make it easy to share
5. **Get feedback** - Improve based on user input
6. **Scale up** - Deploy to cloud for wider access

---

**Ready to share? üöÄ**

See `USER_SETUP_GUIDE.md` for complete documentation!
