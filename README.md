# Data Analysis Engine Agent (V2)

## ğŸ¯ Overview

An **intelligent, multi-layer pipeline system** that analyzes data using natural language prompts. Simply describe what you want, upload your data, and let the AI agent automatically select and execute the right analytical tools.

### âœ¨ Key Features

- ğŸ¤– **Natural Language Interface**: Describe your analysis in plain English
- ğŸ” **Intelligent Tool Selection**: Automatic tool selection with consensus voting
- ğŸ“Š **Multiple Analysis Types**: Anomaly detection, clustering, forecasting, classification, and more
- ğŸ”— **Smart Tool Chaining**: Sequential or parallel tool execution
- ğŸ“ˆ **Real-time Results**: Instant insights with visualizations
- ğŸªµ **Complete Pipeline Visibility**: See exactly what the system is doing at each step

---

## ğŸš€ Quick Start

### For End Users (Easiest Way)

```bash
# 1. Clone the repository
git clone https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP.git
cd Data_Analysis_Engine_Agent_MCP
git checkout DAA_v2

# 2. Start everything with Docker
docker-compose up -d --build

# 3. Open your browser
# Frontend: http://localhost:3001
# API Docs: http://localhost:8080/docs
```

**That's it!** ğŸ‰ Now you can:
1. Upload your CSV file
2. Type what you want (e.g., "detect anomalies")
3. View results and pipeline logs

ğŸ“– **Full Setup Guide**: See [USER_SETUP_GUIDE.md](USER_SETUP_GUIDE.md) for detailed instructions

---

## ğŸ“Š What Can You Analyze?

| Analysis Type | Example Prompts | Use Cases |
|--------------|----------------|-----------|
| **Anomaly Detection** | "detect anomalies", "find outliers" | IoT monitoring, fraud detection |
| **Clustering** | "cluster data", "group similar items" | Customer segmentation, pattern discovery |
| **Forecasting** | "forecast next 7 days", "predict future" | Sales prediction, demand planning |
| **Classification** | "classify data", "predict category" | Churn prediction, risk assessment |
| **Stats Comparison** | "compare statistics", "compare groups" | A/B testing, regional analysis |
| **Feature Engineering** | "engineer features", "create features" | ML preprocessing, data transformation |
| **Incident Detection** | "detect incidents", "find failures" | System monitoring, quality control |

---

## ğŸ—ï¸ Architecture (V2 Pipeline)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INPUT                               â”‚
â”‚  â€¢ CSV Data Upload                                          â”‚
â”‚  â€¢ Natural Language Prompt ("detect anomalies")            â”‚
â”‚  â€¢ Configuration (metric, timestamp, etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: CONTEXT EXTRACTION                                â”‚
â”‚  â€¢ Intent understanding with consensus voting               â”‚
â”‚  â€¢ Goal extraction (anomaly_detection, clustering, etc.)    â”‚
â”‚  â€¢ Data type detection (timeseries, tabular, geospatial)    â”‚
â”‚  â€¢ Confidence scoring                                        â”‚
â”‚  â€¢ Methods: RULE_BASED â†’ ML â†’ LLM (majority vote)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: CHAINING MANAGER (Planning & Decision Making)    â”‚
â”‚  â€¢ Tool selection based on goal                             â”‚
â”‚  â€¢ Execution strategy (single/sequential/parallel)          â”‚
â”‚  â€¢ Conflict detection & resolution                          â”‚
â”‚  â€¢ Dependency management                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: INVOCATION LAYER (Tool Execution)                â”‚
â”‚  â€¢ REST API calls to tool microservices                     â”‚
â”‚  â€¢ Timeout & retry handling                                 â”‚
â”‚  â€¢ Result aggregation                                        â”‚
â”‚  â€¢ Detailed execution logging                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: OUTPUT PREPARATION                                â”‚
â”‚  â€¢ Result formatting                                         â”‚
â”‚  â€¢ Pipeline logs compilation                                â”‚
â”‚  â€¢ UI-friendly response                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER OUTPUT                              â”‚
â”‚  â€¢ Analysis results with visualizations                     â”‚
â”‚  â€¢ Complete pipeline execution logs                         â”‚
â”‚  â€¢ Tool invocation details                                  â”‚
â”‚  â€¢ Downloadable results                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

### Backend (Agent Service)
- **Framework**: FastAPI (Python 3.11+)
- **Features**: Auto-reload, async support, OpenAPI docs
- **Logging**: Loguru with structured logs
- **Observability**: OpenTelemetry ready

### Tool Microservices (8 Services)
- **Containerization**: Docker
- **Communication**: REST APIs
- **Tools**: 
  - anomaly_zscore (Z-Score anomaly detection)
  - clustering (K-means, DBSCAN)
  - classifier_regressor (ML predictions)
  - timeseries_forecaster (ARIMA, Prophet)
  - stats_comparator (Statistical analysis)
  - geospatial_mapper (Geographic analysis)
  - incident_detector (Event detection)
  - feature_engineering (Feature creation)

### Frontend
- **Framework**: React.js
- **Features**: Real-time updates, CSV upload, visualizations
- **Port**: 3001

### Infrastructure
- **Orchestration**: Docker Compose
- **Auto-scaling**: Ready for Kubernetes (KEDA)
- **Load Balancing**: nginx-ready

---

## ğŸ“ Project Structure

```
Data_Analysis_Engine_Agent/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ agent/                      # Main agent service
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI app + V2 pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_extraction/  # Context extraction layer
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ context_extractor.py  # Consensus voting
â”‚   â”‚   â”‚   â”œâ”€â”€ planner/            # Chaining manager
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chaining_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dispatcher/         # Tool invocation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ invocation_layer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ registry/           # Tool registry
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas/            # Pydantic models
â”‚   â”‚   â”‚   â””â”€â”€ observability/      # Tracing & metrics
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ tools/                      # Microservices
â”‚       â”œâ”€â”€ anomaly_zscore/
â”‚       â”œâ”€â”€ clustering/
â”‚       â”œâ”€â”€ classifier_regressor/
â”‚       â”œâ”€â”€ timeseries_forecaster/
â”‚       â”œâ”€â”€ stats_comparator/
â”‚       â”œâ”€â”€ geospatial_mapper/
â”‚       â”œâ”€â”€ incident_detector/
â”‚       â””â”€â”€ feature_engineering/
â”‚
â”œâ”€â”€ frontend/                       # React UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                 # Main component
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ docker-compose.yml              # Orchestration
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ USER_SETUP_GUIDE.md            # Complete setup guide
â”œâ”€â”€ PIPELINE_LOGGING.md            # Logging documentation
â””â”€â”€ FALLBACK_CHAIN.md              # Consensus voting guide
```

---

## ğŸš¦ Getting Started (Development Mode)

### Prerequisites
- Python 3.11+
- Node.js 16+
- Docker & Docker Compose
- Git

### Backend Setup

```bash
# Navigate to agent service
cd services/agent

# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload
uvicorn app.main:app --reload --port 8080
```

### Tool Services Setup

```bash
# From project root
docker-compose up -d --build

# Verify services
docker ps
```

### Frontend Setup

```bash
cd frontend
npm install
npm start
```

**Access Points:**
- Frontend: http://localhost:3001
- Backend API: http://localhost:8080
- API Docs: http://localhost:8080/docs
- Health Check: http://localhost:8080/health

---

## ğŸ“– Documentation

**ğŸ“Œ Choose Your Path:**

| I am a... | Start Here |
|-----------|------------|
| ğŸ‘¤ **End User** (Want to analyze data) | [User Setup Guide](USER_SETUP_GUIDE.md) - Quick start & usage |
| ğŸ’» **Developer** (API integration, technical details) | [Developer Guide](USER_GUIDE.md) - API reference & architecture |
| ğŸš€ **Sharing/Deploying** (Setup for team/cloud) | [How to Share Guide](HOW_TO_SHARE.md) - Deployment options |

**ğŸ“š Additional Resources:**

| Document | Description |
|----------|-------------|
| [USER_SETUP_GUIDE.md](USER_SETUP_GUIDE.md) | Complete installation and usage guide for end users |
| [USER_GUIDE.md](USER_GUIDE.md) | Technical deep-dive, API reference, and developer documentation |
| [HOW_TO_SHARE.md](HOW_TO_SHARE.md) | Distribution and deployment methods |
| [PIPELINE_LOGGING.md](PIPELINE_LOGGING.md) | Pipeline execution logging details |
| [FALLBACK_CHAIN.md](FALLBACK_CHAIN.md) | Consensus voting mechanism |
| [API Docs](http://localhost:8080/docs) | Interactive OpenAPI documentation |

---

## ğŸ¨ Dashboard Features

### 1. Pipeline Execution Logs
Real-time visibility into all pipeline layers:
- ğŸš€ **PIPELINE** (Purple): Overall pipeline events
- ğŸ“‹ **CONTEXT EXTRACTION** (Blue): Intent understanding
- ğŸ”— **CHAINING MANAGER** (Orange): Tool selection & planning  
- âš™ï¸ **INVOCATION LAYER** (Green): Tool execution
- ğŸ“¤ **OUTPUT PREPARATION** (Cyan): Result formatting

### 2. Tool Invocation Details
- Tool display names and status badges
- Execution summaries (rows processed, anomalies found)
- Status messages with detailed information
- Error handling and retry logic

### 3. Analysis Results
- CSV data preview
- Visualizations (charts, graphs)
- Downloadable results
- Tool-specific outputs

---

## ğŸ”§ Configuration

### Environment Variables

```env
# Backend
AGENT_PORT=8080
LOG_LEVEL=INFO

# Features
ENABLE_FALLBACK_CHAIN=true        # Consensus voting
ENABLE_CONSENSUS_VOTING=true

# Optional: LLM Integration
OPENAI_API_KEY=your_key_here
LLM_MODEL=gpt-4
```

### Advanced Features

**Consensus Voting** (Default: Enabled)
- Tries multiple extraction methods (RULE, ML, LLM)
- Uses weighted voting (LLM=2 votes, others=1 vote)
- Returns result with highest agreement
- Adjusts confidence based on consensus level

**Manual Tool Selection**
```json
{
  "context": {
    "task": "analyze data",
    "force_tools": ["anomaly_zscore", "clustering"]
  }
}
```

---

## ğŸ› Troubleshooting

### Common Issues

**Backend won't start:**
```bash
# Check port availability
netstat -ano | findstr :8080

# Change port if needed
uvicorn app.main:app --reload --port 8090
```

**Tool services not available:**
```bash
# Rebuild and restart
docker-compose down
docker-compose up -d --build

# Check logs
docker-compose logs anomaly-zscore
```

**Frontend issues:**
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
npm start
```

**More help:** See [USER_SETUP_GUIDE.md](USER_SETUP_GUIDE.md) troubleshooting section

---

## ğŸ¤ Contributing

We welcome contributions! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines
- Follow existing code style
- Add tests for new features
- Update documentation
- Test with Docker Compose before submitting

---

## ğŸ“Š Example Workflows

### Workflow 1: Detect Anomalies in IoT Sensors
```
1. Upload sensor_data.csv
2. Task: "detect temperature anomalies"
3. Metric: temperature
4. Timestamp: timestamp
5. Key Fields: sensor_id
6. View results: 15 anomalies detected in 1000 rows
```

### Workflow 2: Customer Segmentation
```
1. Upload customer_data.csv
2. Task: "cluster customers into 4 groups"
3. Metric: purchase_amount, frequency
4. Key Fields: customer_id
5. View results: 4 clusters with characteristics
```

### Workflow 3: Sales Forecasting
```
1. Upload sales_history.csv
2. Task: "forecast sales for next 7 days"
3. Metric: sales_amount
4. Timestamp: date
5. Key Fields: store_id
6. View results: Predictions with confidence intervals
```

---

## ğŸ” Security

For production deployments:
- Enable JWT authentication
- Configure HTTPS/SSL
- Implement rate limiting
- Add data encryption
- Set up audit logging
- Configure CORS properly

See [USER_SETUP_GUIDE.md](USER_SETUP_GUIDE.md) for security details.

---

## ğŸ“ Support

- **Documentation**: Check docs in this repo
- **Issues**: [GitHub Issues](https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP/issues)
- **Logs**: 
  - Backend: `services/agent/logs/agent_v2.log`
  - Docker: `docker-compose logs <service-name>`

---

## ğŸ“‹ System Requirements

**Minimum:**
- CPU: 2 cores
- RAM: 4GB
- Disk: 2GB free
- OS: Windows 10+, Linux, macOS

**Recommended:**
- CPU: 4+ cores
- RAM: 8GB+
- Disk: 5GB+ free

---

## ğŸš€ What's New in V2

âœ… **Consensus Voting**: Multiple extraction methods with majority voting  
âœ… **Enhanced Logging**: Complete pipeline visibility with color-coded layers  
âœ… **Improved UI**: Better tool invocation display with execution summaries  
âœ… **Conflict Resolution**: Smart conflict detection and auto-resolution  
âœ… **Better Accuracy**: Higher confidence in intent extraction  
âœ… **Zero False Positives**: Removed incorrect conflict checks  

---

## ğŸ“ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

Built with:
- FastAPI for high-performance APIs
- React for responsive UI
- Docker for containerization
- OpenTelemetry for observability

---

**Version**: v2.0.0  
**Last Updated**: November 4, 2025  
**Maintained by**: SwetaAIS2024

---

## ğŸ¯ Quick Links

- ğŸ“– [Complete Setup Guide](USER_SETUP_GUIDE.md)
- ğŸªµ [Pipeline Logging Guide](PIPELINE_LOGGING.md)
- ğŸ”— [Consensus Voting Guide](FALLBACK_CHAIN.md)
- ğŸ”§ [API Documentation](http://localhost:8080/docs)
- ğŸ› [Report Issues](https://github.com/SwetaAIS2024/Data_Analysis_Engine_Agent_MCP/issues)

---

## ğŸ§± Core Components

### 1. Agent API Layer
**Tech:** Python (FastAPI/Flask), Docker  
**Purpose:** Exposes REST endpoints (e.g., `/analyze`) to accept input data and context

**Responsibilities:**
- Parse & validate request payloads
- Forward to Router for tool assignment
- Manage response formatting, chaining, and error handling
- Emit job/run events for visualization dashboards

### 2. Router Module
**Modes:**
- Rule-Based Routing: Based on input types, keywords, metadata
- ML-Based Routing: Intent classification using ML/LLM
- Hybrid Routing: Fast rules + fallback to model-driven dispatch

**Responsibilities:**
- Decide best-fit MCP tool or tool chain
- Send routing metadata to Dispatcher
- Generate trace identifiers and step metadata for visualization

### 3. Tool Dispatcher
**Function:** Orchestrates tool invocations

**Protocols Supported:**
- REST (default)
- gRPC (for low-latency/high-throughput)
- Kafka/RabbitMQ (for async workloads)

**Responsibilities:**
- Handles retries, timeouts
- Resolves tool endpoint from Tool Registry
- Loads authentication headers + payloads
- Publishes step progress and results for visualization and tracing

### 4. Tool Chaining Manager
**Purpose:** Executes tool pipelines

**Approach:**
- DAG-based chaining (e.g., Anomaly Detection -> Clustering)
- Agent-guided dynamic chaining

**Responsibilities:**
- Manage data hand-off between tools
- Track intermediate results and state
- Report stage transitions to the visualization subsystem

### 5. Tool Registry
**Storage:** Local JSON file or database

**Fields:**
- Tool name, task type, supported data types
- Endpoint URL & communication protocol
- Version, metadata, health status
- Visualization metadata: category, icon, color code, owner

### 6. MCP Tool Interface
**Standardized Schema:**
```json
{
	"input": { ... },
	"context": { ... }
}
```
**Returns:**
```json
{
	"status": "success",
	"output": { ... },
	"meta": { ... }
}
```

---

*This architecture enables scalable, flexible, and intelligent data analysis for diverse and demanding workloads.*

---

## Visualization & Monitoring Layer

**Purpose:** Provide full transparency into running processes and tool interactions.

**Components:**
- **Run Service / Jobs API:** Tracks all runs, jobs, steps, and statuses; exposes REST + WebSocket endpoints (e.g., `/v1/runs`, `/ws/runs/{id}`)
- **Dashboard UI:** Shows tool catalog, live runs, DAG visualizer (tool chaining), job progress, and metrics
- **Tracing:** OpenTelemetry + Jaeger/Tempo for distributed traces
- **Logs:** ELK/OpenSearch for structured logs (linked to runs)
- **Metrics:** Prometheus + Grafana for latency, throughput, error rate
- **Lineage:** OpenLineage/Marquez integration for datasetâ€“toolâ€“output provenance

**User View:**
- Tool catalog with capabilities, schema, and status
- Real-time run status, progress bars, ETA, per-step logs, and trace links
- DAG view showing current pipeline execution flow

---

## Communication & Throughput Management

**Options:**
- REST (development, small-scale)
- gRPC (binary RPCs, high throughput)
- Kafka (buffered, async tasks)

**Concurrency:**
- Python asyncio / Celery for parallel calls
- K8s for container auto-scaling
- KEDA for queue-based scaling

---

## Security Layer

**Auth:**
- JWT-based access control
- HMAC signing for internal tool calls

**Transport:**
- TLS encryption for REST/gRPC
- Kafka: TLS + SASL

**Audit Logging:**
- Request, tool, user, timestamp, result status
- Integrated into visualization UI for admin access

---

## Observability & Governance

**Tracing:** OpenTelemetry spans per request and tool
**Metrics:** Prometheus collectors for latency, throughput, queue lag
**Logging:** Structured, tenant-aware JSON logs
**SLOs & Alerts:** Alertmanager for anomalies and health checks

**Governance:**
- Versioning, tool lifecycle tracking
- Canary releases and shadow runs
- UI displays deprecation notices and tool change logs

---

## ğŸ—ƒ Data & Task Support

**Input Types:**
- Tabular (CSV, Excel, SQL result)
- Text (incident reports, logs)
- JSON/XML (API or IoT device input)
- Images (traffic cams)
- Geo (GeoJSON, GPS points)

**Supported Tasks:**
- Anomaly Detection
- Incident Detection
- Time-Series Forecasting
- Descriptive Stats & Comparison
- Classification / Regression
- Clustering & Feature Engineering
- Geospatial Mapping & Analysis

---

## Deployment Notes

- Containerized with Docker for each tool and core module
- Use Docker Compose or Kubernetes for orchestration
- Includes visualization stack (Grafana, Jaeger, ELK) and UI dashboard
- Designed to plug into larger systems as a callable API service
- Future-proofed for more advanced ML planning agents (e.g., LLM planner)

---

## Next Steps

---

## Frontend Visualization

This project includes a React-based frontend for uploading datasets, running anomaly detection, and visualizing results.

### How to Run the Frontend

1. Open a terminal and navigate to the `frontend` folder:
	```
	cd frontend
	```
2. Install dependencies:
	```
	npm install
	```
3. Start the development server:
	```
	npm start
	```
4. Open your browser and go to:
	```
	http://localhost:3000
	```

**Note:** Make sure the MCP agent backend is running at `http://localhost:8080` before using the frontend.

### Features
- Upload CSV dataset
- Run anomaly detection
- View detected anomalies in a table and chart
- See summary statistics

You can extend the UI for more tools, real-time updates, and advanced visualizations as needed.

1. Scaffold base API + agent logic
2. Implement router (rule-based first)
3. Add 3â€“5 MCP tools with REST endpoints
4. Integrate Run Service + WebSocket for real-time progress
5. Add UI layer for visualization (DAG, runs, logs)
6. Package with Docker Compose for local testing
7. Extend with gRPC + Kafka for async cases


# Create a virtual environment named .venv
python -m venv t_venv

# Activate the virtual environment
t_venv\Scripts\activate

# (Optional) Upgrade pip
python -m pip install --upgrade pip

# (Optional) Install dependencies from requirements.txt
pip install -r requirements.txt